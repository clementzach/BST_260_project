---
title: "EDA for P1"
author: "Addison McGhee"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, message = F)
```

```{r, include=F}
library(caret)
library(e1071)
library(gam)
library(ggthemes)
library(lubridate)
library(magrittr)
library(pander)
library(pROC)
library(pscl)
library(randomForest)
library(rpart)
library(splitstackshape)
library(tidyverse)
theme_set(theme_economist())
options(digits = 3)
theme_update(plot.title = element_text(hjust = 0.5))
```

```{r, include=F, eval=F}
injured = read.csv("../../Data/all_injuries_clean.csv") # load data
```

```{r, include=F, eval=F}
fix_nfl_names <- function(x){ #https://rdrr.io/github/papagorgio23/bettoR/src/R/fix_nfl_names.R
  x[grep("Arizona Cardinals", x, ignore.case=TRUE)] <- "ARI"
  
  x[grep("Atlanta Falcons", x, ignore.case=TRUE)] <- "ATL"

  x[grep("Baltimore Ravens", x, ignore.case=TRUE)] <- "BAL"

  x[grep("Buffalo Bills", x, ignore.case=TRUE)] <- "BUF"

  x[grep("Carolina Panthers", x, ignore.case=TRUE)] <- "CAR"

  x[grep("Chicago Bears", x, ignore.case=TRUE)] <- "CHI"

  x[grep("Cincinnati Bengals", x, ignore.case=TRUE)] <- "CIN"

  x[grep("Cleveland Browns", x, ignore.case=TRUE)] <- "CLE"

  x[grep("Dallas Cowboys", x, ignore.case=TRUE)] <- "DAL"

  x[grep("Denver Broncos", x, ignore.case=TRUE)] <- "DEN"

  x[grep("Detroit Lions", x, ignore.case=TRUE)] <- "DET"

  x[grep("Green Bay Packers", x, ignore.case=TRUE)] <- "GB"

  x[grep("Houston Texans", x, ignore.case=TRUE)] <- "HOU"

  x[grep("Indianapolis Colts", x, ignore.case=TRUE)] <- "IND"

  x[grep("Jacksonville Jaguars", x, ignore.case=TRUE)] <- "JAX"

  x[grep("Kansas City Chiefs", x, ignore.case=TRUE)] <- "KC"

  x[grep("Miami Dolphins", x, ignore.case=TRUE)] <- "MIA"

  x[grep("Minnesota Vikings", x, ignore.case=TRUE)] <- "MIN"

  x[grep("New England Patriots", x, ignore.case=TRUE)] <- "NE"

  x[grep("New Orleans Saints", x, ignore.case=TRUE)] <- "NO"

  x[grep("New York Jets", x, ignore.case=TRUE)] <- "NYJ"

  x[grep("New York Giants", x, ignore.case=TRUE)] <- "NYG"

  x[grep("Las Vegas Raiders", x, ignore.case=TRUE)] <- "OAK"

  x[grep("Philadelphia Eagles", x, ignore.case=TRUE)] <- "PHI"

  x[grep("Pittsburgh Steelers", x, ignore.case=TRUE)] <- "PIT"

  x[grep("Los Angeles Chargers", x, ignore.case=TRUE)] <- "LAC"

  x[grep("Los Angeles Rams", x, ignore.case=TRUE)] <- "LAR"

  x[grep("San Francisco 49ers", x, ignore.case=TRUE)] <- "SF"

  x[grep("Seattle Seahawks", x, ignore.case=TRUE)] <- "SEA"

  x[grep("Tampa Bay Buccaneers", x, ignore.case=TRUE)] <- "TB"

  x[grep("Tennessee Titans", x, ignore.case=TRUE)] <- "TEN"

  x[grep("Washington Redskins", x, ignore.case=TRUE)] <- "WAS"

  return(x)
}

age <- function(dob, age.day = today(), units = "years", floor = TRUE) {
    calc.age = interval(dob, age.day) / duration(num = 1, units = units)
    if (floor) return(as.integer(floor(calc.age)))
    return(calc.age)
}
```

&nbsp;


```{r, include=F, eval= F}
### Select columns
injured = injured %>% 
  mutate(short_team_name = fix_nfl_names(full_team), injury = rep(1, nrow(injured))) %>%
  mutate(total_injuries = dplyr::select(injured, head, shoulder, upper_torso, lower_torso, arm, hand, leg, foot) %>% rowSums) %>% 
  filter(total_injuries != 0)

```

&nbsp;


```{r, include=F, eval=F}
### Load demographic data

players_dat = read.csv("../../Data/all_player_demographic_clean.csv")

players = players_dat %>%
  mutate(short_team_name = fix_nfl_names(full_team),
         age = age(ymd(birthdate)), bmi = (weight_pounds / height_inches^2)*703) %>%
  mutate(age = age - (2021 - year))

```

&nbsp;

```{r, include=F, eval=F}
### Merge with injury data

injuries = injured %>% left_join(players, by = c("name", "full_team", "short_team_name", "year", "team")) %>%
  filter(position_id != "") %>%
  mutate(injury = factor(injury), position_id = factor(position_id)) %>%
  filter(age < 50)

players = players %>% left_join(injured, by = c("name", "full_team", "short_team_name", "year", "team")) %>%
  mutate(injury = factor(ifelse(is.na(injury), 0, 1))) %>%
  filter(position_id != "") %>%
  mutate(injury = factor(injury), position_id = factor(position_id)) %>%
  filter(age < 50)


write.csv(injuries, "../../Data/data_for_plots/Q1/injury_dat_q1.csv", row.names = F)
write.csv(players, "../../Data/data_for_plots/Q1/players_dat_q1.csv", row.names = F)
```

&nbsp;

### Load data from .csv files saved in Data folder
```{r}
injuries = read.csv("../../Data/data_for_plots/Q1/injury_dat_q1.csv", stringsAsFactors = T) %>% 
  mutate(injury = factor(injury))

players = read.csv("../../Data/data_for_plots/Q1/players_dat_q1.csv", stringsAsFactors = T) %>% mutate(injury = factor(injury))
```

&nbsp;

# P1

### Q1: Which positions are most at-risk for injuries? The table below contains descriptive statistics calculated for each position over multiple seasons (2009 - Present). 

```{r}
# set up table
number_injury = injuries %>% 
  group_by(position_id, year) %>% 
  count()

number_injury %>% group_by(position_id) %>% 
  summarise(mean = mean(n), 
            sd = sd(n), 
            Q1 = quantile(n, probs = 0.25),
            median = median(n),
            Q3 = quantile(n, probs = 0.75)
  ) %>% 
  pander
```                                                   

This table does not account for the number of people who play each position. Let's make a new table that may be more useful.


```{r, include=F, eval=F}       
## Deprecated plot over time

number_injury %>% ggplot(aes(year, n, color = position_id)) +
                    geom_line() +
                    xlab("Year") +
                    ylab("# of Players Injured") +
                    scale_color_discrete(name = "Position") +
                    ggtitle("Number of Players Injured by Position (2009 - Present)")
```

&nbsp;


```{r, include=F, eval=F}       
# Deprecated log transform of plot over time

number_injury %>% ggplot(aes(year, n, color = position_id)) +
                    geom_line() +
                    xlab("Year") +
                    ylab(bquote("# of Players Injured ("*log_10* "scale)")) +
                    scale_color_discrete(name = "Position") +
                    ggtitle("Number of Players Injured (log_10 scale) by Position (2009 - Present)") +
                    scale_y_log10()
```


### avg. # of injuries per position per season, among people who get injured
```{r}
# (meaning if I am a wide receiver, how many injuries can I expect to get this season?)

# total injuries per player per year
exp_injuries <- players %>% 
  filter(!is.na(total_injuries)) %>% # remove players with no injury (NA's)
  group_by(position_id) %>% 
  summarise(avg_total_injuries = sum(total_injuries)/n(),
            SD = sd(total_injuries), 
            Q1 = quantile(total_injuries, probs = 0.25),
            Median = median(total_injuries),
            Q3 = quantile(total_injuries, probs = 0.75),
            Min = min(total_injuries),
            Max = max(total_injuries)
  )

exp_injuries %>% pander


# code for plot over time
exp_injuries_byYear <- players %>% 
  filter(!is.na(total_injuries)) %>% # remove players with no injury (NA's)
  group_by(position_id, year) %>% 
  summarise(avg_total_injuries = sum(total_injuries)/n())# trying to divide by number of players to get avg. for each player
# plot
exp_injuries_byYear %>%
  ggplot(aes(x = year, y = avg_total_injuries, color = position_id)) +
  geom_line() +
  ggtitle("Average Number of Injuries per Season among Injured Players") +
  ylim(0.7, 2.3) +
  xlab("Season") +
  ylab("Average Total Injuries") +
  scale_x_continuous(breaks = seq(2008, 2020, by = 2)) +
  scale_color_discrete(name = "Position", labels = c("Defense", "Kicker", "Offensive Line", "Punter", 
                                                     "Quarterback", "Running Back", "Tight End", "Wide Receiver")) 
```

Note: we can't include people who were not injured in this calculation, because we have a complete dataset of injured players, but we do not have a complete dataset of non-injured players.

&nbsp;


### We now will switch to our second analysis (Machine Learning)
* We will perform logistic regression, kNN, and random forest using the binary outcome `injury`. 
* We will then compare the results of the different methods based on accuracy, sensitivity/specificity, and ROC/AUC.
* Logical potential predictors: position_id, height_inches, weight_pounds, game_starter, age, bmi

&nbsp;

### Our first step is to check for missing data in our predictors and outcome
```{r}
apply(players %>% 
        dplyr::select(injury, position_id, height_inches, weight_pounds, game_starter, age, bmi, year), 
      2, function(x) sum(is.na(x))) %>% pander

# remove the 13 people with missing height/weight (much less than 5% of each column)
players <- players %>%
  drop_na(height_inches)
```


```{r, include=F, eval=F}
### What is the distribution of injuries by position?

players$position_id =
  fct_relevel(players$position_id, c("DEF", "OL", "WR", "RB", "TE", "QB", "K", "P"))

players %>%
  ggplot(aes(position_id, fill = injury)) +
    geom_bar(position = "dodge") +
    xlab("Position") +
    ylab("# of Injured Players") +
    ggtitle("Injured Players by Position in 2018") +
    scale_fill_manual(name = "Injury", guide = guide_legend(reverse=TRUE), values = c("#619CFF", "#F8766D"))

```



### What is the distribution of BMI for the injured vs non-injured players?
```{r}
players %>% ggplot(aes(injury, bmi)) +
              geom_boxplot() +
              xlab("Injury Status") +
              ylab("BMI") +
              ggtitle("BMI for Injured/Non-Injured Players")
```

&nbsp;

```{r}
players %>% ggplot(aes(injury, age)) +
              geom_boxplot() +
              xlab("Injury Status") +
              ylab("Age") +
              ggtitle("Age of Injured/Non-Injured Players")
```


**The absence of any significant difference between groups suggests that `bmi` will not be a significiant predictor in our Logistic model. However, the boxplots for `age` suggest the opposite, mainly that `age` may be a sigificant predictor.**


&nbsp;


```{r, eval=F, include=F}
### What is the distribution of `injury` by `age` in 2018?

injuries %>% filter(year == 2018) %>%
             ggplot(aes(age)) +
               geom_histogram(binwidth = 0.5) +
               xlab("Age") +
               ylab("# of Players Injured") +
               ggtitle("# of Players Injured by Age in 2018") +
               scale_x_continuous(breaks = seq(0, 50, 1))
```

&nbsp;


```{r, include=F, eval=F}
## Try side by side bars for `age`

players %>% ggplot(aes(age, fill = injury)) +
              geom_histogram(binwidth = 0.5, position = "dodge") +
              xlab("Age") +
              ylab("# of Players Injured") +
              ggtitle("# of Injured Players by Age in 2018") +
              scale_x_continuous(breaks = seq(0, 50, 1)) +
              scale_fill_manual(name = "Injury",
                                guide = guide_legend(reverse=TRUE), values = c("#619CFF", "#F8766D"))

```

&nbsp;


```{r, include=F, eval=F}
## Try scaling by number of players in each age group

age_group_size = players %>% group_by(age) %>% count() # get size of each age group

injured_age_group_size = players %>% # get size of age group for injured players only
                          filter(injury == 1) %>%
                          group_by(age) %>%
                          count()

# Some age groups have no injuries, so the vector lengths differ
group_size = age_group_size %>% filter(age %in% injured_age_group_size$age) %>% .$n

injured_age_group_size$scaled_n = injured_age_group_size$n / group_size # Scale by group size
head(injured_age_group_size)

injured_age_group_size %>% ggplot(aes(age, scaled_n)) +
                             geom_col() +
                             xlab("Age") +
                             ylab("Percentage") +
                             ggtitle("Percentage of Players Injured by Age Group") +
                             scale_x_continuous(breaks = seq(0, 50, 1)) +
                             scale_fill_manual(name = "Injury",
                               guide = guide_legend(reverse=TRUE), values = c("#619CFF", "#F8766D"))

```

&nbsp;

### Logistic regression (model only; check for significant coefficients)

```{r}
players = players %>% mutate(position_id = relevel(position_id, ref = "TE"))

logi_fit = glm(data = players, injury ~ position_id + bmi + age, family = binomial())
pander(summary(logi_fit))

```

&nbsp;

### The older players could be skewing the results! The odds of getting injured for players above 40 is very high, mainly because there are very players who play at or above this age.

&nbsp;

### Let's remove the players above 40 y/o and see if `age` is still significant

```{r}
logi_fit2 = glm(data = players %>% filter(age < 40), injury ~ position_id + bmi + age, family = binomial())
pander(summary(logi_fit2))
```

&nbsp;

### Let's try Quadratic Age, too (may not be a linear relationship with injuries)
```{r}
players = players %>% mutate(age_sq = age^2)

logi_fit3 = glm(data = players %>% filter(age < 40), injury ~ position_id + bmi + age + age_sq, family = binomial())
pander(summary(logi_fit3))

```

**age squared is significant**

&nbsp;

# Machine Learning and Prediction

&nbsp;

### Create training and test set

```{r}
set.seed(1)

x <- stratified(players, "injury", 0.7, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- players[-train_index,]

dim(train_set)
dim(test_set)
```

&nbsp;

#### NOTE: Accuracy improved after removing `bmi` as a predictor

### Logistic regression 

#### Stepwise model selection to choose best covariates

```{r}
# baseline logistic regression model (intercept only)
logi_fit_all <- glm(data = players, injury ~ 1, family = binomial())
summary(logi_fit_all)

# forward selection
logi_step_fit <- logi_fit_all %>% 
  step(direction = "forward", data = players, 
       scope = (~ position_id + height_inches + weight_pounds + game_starter + 
                  age + I(age^2) + bmi + year))
summary(logi_step_fit)
```

**AIC forward selection gives us game_starter (yes/no), position, weight, age, and age squared as covariates.** 


#### Now we have our covariates--fit model with training data

```{r, logistic regression}
logi_fit = glm(data = train_set, family = binomial(),
               injury ~ position_id + weight_pounds + game_starter + age + I(age^2) + year)

p_hat_logit = predict(logi_fit, newdata = test_set, type = "response") # get predicted probabilities

y_hat_logit <- as.factor(ifelse(p_hat_logit > 0.5, 1, 0)) # set threshold of 0.5

confusionMatrix(data = y_hat_logit, 
                reference = test_set[["injury"]], positive = "1") # generate confusion matrix

```


&nbsp;



&nbsp;

### k-Nearest Neighbors

#### Use 2-fold cross-validation to find optimal `k` (achieves the highest accuracy for 0.5 cut-off)
```{r, eval=F}
#### Heather's code ####

# control <- trainControl(method='cv', number=2, p=.5)
# dat2 <- mutate(dat, label=as.factor(label)) %>%
#   select(label,X_1,X_2)
# res <- train(label ~ .,
#              data = dat2,
#              method = "knn",
#              trControl = control,
#              tuneLength = 1, # How fine a mesh to go on grid
#              tuneGrid=data.frame(k=seq(3,151,2)),
#              metric="Accuracy")
# plot(res)

set.seed(1)

control <- trainControl(method = 'cv', number = 2, p = .5) # cut-off of 0.5

players2 <- mutate(players, label = injury) %>%
  dplyr::select(label, position_id, weight_pounds, game_starter, age)

res <- train(label ~ .,
             data = players2,
             method = "knn",
             trControl = control,
             tuneLength = 1, # How fine a mesh to go on grid
             tuneGrid = data.frame(k = seq(3, 151, 2)),
             metric = "Accuracy")

res$bestTune # best choice of 'k'
plot(res)
```

```{r kNN}
knn_fit = knn3(data = train_set, injury ~ position_id + age + I(age^2) + game_starter + 
                 weight_pounds + year, 
               k = 71) # fit kNN using optimal

f_hat_knn = predict(knn_fit, newdata = test_set)[ , 2] # get probability for injury

y_hat_knn = as.factor(ifelse(f_hat_knn > 0.5, 1, 0))

confusionMatrix(data = y_hat_knn, 
                reference = test_set$injury, positive = "1") # generate confusion matrix

# plots with 2 dimensions, color = injured vs. not injured to see "neighbors"
players %>%
  ggplot(aes(x = age, y = weight_pounds, color = injury)) +
  geom_point() +
  ggtitle("Visualizing Injured vs. Non-Injured Characteristics") +
  xlab("Age") +
  ylab("Weight (lbs)") +
  scale_color_discrete(name = "Injury")

players %>%
  ggplot(aes(x = weight_pounds, y = game_starter, color = injury)) +
  geom_point() +
  ggtitle("Visualizing Injured vs. Non-Injured Characteristics") +
  xlab("Weight (lbs)") +
  ylab("Game Starter (Yes/No)") +
  scale_color_discrete(name = "Injury")

players %>%
  ggplot(aes(x = weight_pounds, y = position_id, color = injury)) +
  geom_point() +
  ggtitle("Visualizing Injured vs. Non-Injured Characteristics") +
  xlab("Weight (lbs)") +
  ylab("Position") +
  scale_color_discrete(name = "Injury")
```

&nbsp;

### Random Forest

```{r Random Forest}
rf_fit = randomForest(injury ~ position_id + age + I(age^2) + game_starter + weight_pounds + year,
                      data = train_set) # fit random forest using training set

rf_fit

p_hat_rf = predict(rf_fit, newdata = test_set, type = "prob")[,2] # get predicted probabilities

y_hat_rf = as.factor(ifelse(p_hat_rf > 0.5, 1, 0)) # use 0.5 cutoff to get estimates

confusionMatrix(data = y_hat_rf, 
                reference = test_set[["injury"]], positive = "1") # generate confusion matrix

```

&nbsp;

&nbsp;

### Plot ROCs

```{r, message=FALSE}
roc_logi = roc(test_set[["injury"]], p_hat_logit) # ROC curve creation
roc_knn = roc(test_set[["injury"]], f_hat_knn)
roc_rf = roc(test_set[["injury"]], p_hat_rf)

ggroc(list("Logistic Regression" = roc_logi, "kNN, k = 71" = roc_knn, "Random Forest" = roc_rf)) +
  theme(legend.title = element_blank()) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color = "black", linetype = "dashed") +
  ggtitle("ROC Curves") +
  xlab("Specificity") +
  ylab("Sensitivity") 
```


```{r}
auc(roc_logi) # calculate AUC for each ROC curve
auc(roc_knn)
auc(roc_rf)
```

### So which model is the best?

Accuracy:  
- logistic regression: 0.638
- kNN: 0.583
- random forest: 0.636

Logistic regression has the highest accuracy.

AUC:  
- logistic regression: 0.701
- kNN: 0.607
- random forest: 0.691

Logistic regression has the highest AUC.

It's clear that kNN is the poorest performing model, so we'll consider logistic regression and random forest.

logistic regression:  
sensitivity: 0.675          
specificity: 0.600   

random forest:  
Sensitivity: 0.632         
Specificity: 0.640

The random forest is slightly more balanced in terms of sensitivity/specificity.

Logistic regression has the highest accuracy and the highest AUC of all 3 model. It also provides us with clearly interpretable odds ratios for the risk of injury. For these reasons, we'll choose logistic regression as the best model.


&nbsp;
